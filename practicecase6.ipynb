{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "No. 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2021-02.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-02.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Green Taxi\n",
    "df_gr = spark.read.parquet('green_tripdata_2021-02.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Yellow Taxi\n",
    "df_ye = spark.read.parquet('yellow_tripdata_2021-02.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since column for date in green and yellow taxi has differ name, we change one of them into the other\n",
    "df_ye = df_ye.withColumnRenamed('tpep_pickup_datetime', 'pickup_datetime').withColumnRenamed('tpep_dropoff_datetime', 'dropoff_datetime')\n",
    "df_gr = df_gr.withColumnRenamed('lpep_pickup_datetime', 'pickup_datetime').withColumnRenamed('lpep_dropoff_datetime', 'dropoff_datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating tmp view for green taxi and yellow taxi trip\n",
    "df_gr.createOrReplaceTempView('green')\n",
    "df_ye.createOrReplaceTempView('yellow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_colums = []\n",
    "\n",
    "yellow_columns = set(df_ye.columns)\n",
    "\n",
    "for col in df_gr.columns:\n",
    "    if col in yellow_columns:\n",
    "        common_colums.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VendorID',\n",
       " 'pickup_datetime',\n",
       " 'dropoff_datetime',\n",
       " 'store_and_fwd_flag',\n",
       " 'RatecodeID',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'passenger_count',\n",
       " 'trip_distance',\n",
       " 'fare_amount',\n",
       " 'extra',\n",
       " 'mta_tax',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'improvement_surcharge',\n",
       " 'total_amount',\n",
       " 'payment_type',\n",
       " 'congestion_surcharge']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_colums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = df_gr.select(common_colums).unionAll(df_ye.select(common_colums))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1436281"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.createOrReplaceTempView('full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To remove duplicated row if there is any\n",
    "df_full = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    DISTINCT(*)\n",
    "FROM\n",
    "    full\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1436281"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the pickup_datetime column from TimeStamp type to DateTime type\n",
    "question1 =  df_full.withColumn(\"pickup_datetime\" , to_date(df_full['pickup_datetime'])) \\\n",
    "            .select('pickup_datetime') \\\n",
    "            .filter((df_full['pickup_datetime'] >= '2021-02-15')&(df_full['pickup_datetime'] < '2021-02-16')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45497"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. How many taxi trips were there on February 15?\n",
    "Ans : 45497\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "No. 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'max(trip_distance)[values]'>"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(df_full['trip_distance'])\n",
    "max(col('col1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "question2 = df_full.withColumn(\"pickup_datetime\" , to_date(df_full['pickup_datetime']))\\\n",
    "            .select(['pickup_datetime','trip_distance'])\\\n",
    "            .where(\"pickup_datetime >= '2021-02-01' \")\\\n",
    "            .groupby(F.col('pickup_datetime')).agg(F.max('trip_distance').alias('trip_maxdistance'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------+\n",
      "|pickup_datetime|trip_maxdistance|\n",
      "+---------------+----------------+\n",
      "|     2021-02-15|           52.89|\n",
      "|     2021-02-02|           73.24|\n",
      "|     2021-02-26|        90796.21|\n",
      "|     2021-02-21|        47327.62|\n",
      "|     2021-02-05|        91134.16|\n",
      "|     2021-02-10|         60382.7|\n",
      "|     2021-02-01|           38.89|\n",
      "|     2021-02-06|           48.35|\n",
      "|     2021-02-19|           75.81|\n",
      "|     2021-03-02|           28.34|\n",
      "|     2021-02-20|       188054.03|\n",
      "|     2021-02-08|       186617.92|\n",
      "|     2021-02-09|        89416.24|\n",
      "|     2021-02-11|        43174.56|\n",
      "|     2021-02-17|       140145.44|\n",
      "|     2021-02-25|        50422.63|\n",
      "|     2021-03-01|         72499.7|\n",
      "|     2021-02-27|           91.05|\n",
      "|     2021-02-24|        90073.44|\n",
      "|     2021-02-18|        29501.25|\n",
      "|     2021-02-14|       115928.92|\n",
      "|     2021-02-12|        66659.27|\n",
      "|     2021-02-22|           55.87|\n",
      "|     2021-02-03|       186079.73|\n",
      "|     2021-02-13|        54381.65|\n",
      "|     2021-02-23|            79.3|\n",
      "|     2021-02-16|       221188.25|\n",
      "|     2021-02-07|       186510.67|\n",
      "|     2021-02-28|         29486.5|\n",
      "|     2021-02-04|       102620.98|\n",
      "+---------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question2.show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. Find the longest trip for each day ?\n",
    "Ans : (Table Above)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "No. 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2021-02.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhv = spark.read.parquet('fhv_tripdata_2021-02.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|dispatching_base_num|count|\n",
      "+--------------------+-----+\n",
      "|              B00856|35077|\n",
      "|              B01312|33089|\n",
      "|              B01145|31114|\n",
      "|              B02794|30397|\n",
      "|              B03016|29794|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question3 = df_fhv.groupBy(\"dispatching_base_num\").count() \\\n",
    "            .orderBy(F.col('count').desc()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3.  Find Top 5 Most frequent `dispatching_base_num` ?\n",
    "Ans : (Table Above)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "No. 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-----+\n",
      "|PUlocationID|DOlocationID|count|\n",
      "+------------+------------+-----+\n",
      "|       206.0|       206.0| 2374|\n",
      "|       221.0|       206.0| 2112|\n",
      "|       129.0|       129.0| 1902|\n",
      "|         7.0|         7.0| 1829|\n",
      "|       179.0|       179.0| 1736|\n",
      "+------------+------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question4 = df_fhv.where(\"PUlocationID IS NOT NULL AND DOlocationID IS NOT NULL\") \\\n",
    "            .groupBy([\"PUlocationID\",'DOlocationID']) \\\n",
    "            .count() \\\n",
    "            .orderBy(F.col('count').desc()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "4.  Find Top 5 Most common location pairs (PUlocationID and DOlocationID) ?\n",
    "Ans : (Table Above)\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandasDF = df_gr.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandasDF.to_csv('phasmopobia.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_gr.coalesce(1).write.parquet('revenue/', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.conf import SparkConf\n",
    "# from pyspark.context import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_gr.coalesce(1).write.option(\"header\",\"true\").format(\"csv\").save(\"\\IYKRA\\apachespark\\code\\csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_gr.write.parquet(\"P\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# BQ_PROJECT_ID = \"feisty-outlet-362916\"\n",
    "# DATASET_ID = \"practicecase_6\"\n",
    "# TABLE_NAME = \"Question1\"\n",
    "\n",
    "# KEY_FILE = \"/.google/credentials/google_credentials.json\" # When not on GCP\n",
    "# STAGING_BUCKET = \"fellowship7\"              # Intermediate JSON files\n",
    "# DATASET_LOCATION = \"US\"                    # Location for dataset creation\n",
    "\n",
    "# # Start session and reference the JVM package via py4j for convienence\n",
    "# session = SparkSession.builder.getOrCreate()\n",
    "# bigquery = session._sc._jvm.com.samelamin.spark.bigquery\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession \\\n",
    "#     .builder \\\n",
    "#     .master('local') \\\n",
    "#     .appName('spark-read-from-bigquery') \\\n",
    "#     .config('spark.jars.packages','com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.22.0,com.google.cloud.bigdataoss:gcs-connector:hadoop3-1.9.5,com.google.guava:guava:r05') \\\n",
    "#     .config('spark.jars','guava-11.0.1.jar,gcsio-1.9.0-javadoc.jar') \\\n",
    "#     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_gr.write.format('bigquery') \\\n",
    "#         .mode(\"overwrite\") \\\n",
    "#         .option(\"credentialsFile\", \"creds.json\") \\\n",
    "#         .option('table', \"feisty-outlet-362916.practicecase_6.df_gr_tested\") \\\n",
    "#         .option(\"temporaryGcsBucket\",\"fellowship7\") \\\n",
    "#         .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_gr.write.format('bigquery') \\\n",
    "#         .mode(\"overwrite\") \\\n",
    "#         .option(\"credentialsFile\", \"creds.json\") \\\n",
    "#         .option('table', table) \\\n",
    "#         .option(\"temporaryGcsBucket\",bucket) \\\n",
    "#         .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  spark = SparkSession \\\n",
    "#         .builder \\\n",
    "#         .appName(appName) \\\n",
    "#         .config(conf=spark_conf) \\\n",
    "#         .config('spark.jars.packages', 'com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.22.0') \\\n",
    "#         .config('spark.jars.packages','com.google.cloud.bigdataoss:gcsio:1.5.4') \\\n",
    "#         .config('spark.jars', 'gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar,spark-bigquery-with-dependencies_2.12-0.21.1.jar,spark-bigquery-latest_2.11.jar') \\\n",
    "#         .config('spark.jars', 'postgresql-42.2.23.jar,bigquery-connector-hadoop2-latest.jar') \\\n",
    "#         .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving the data to BigQuery\n",
    "# df_gr.write.format('bigquery') \\\n",
    "#   .option('table', 'practicecase_6.wordcount_output') \\\n",
    "#   .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare the bigquery context\n",
    "# bq = bigquery.BigQuerySQLContext(session._wrapped._jsqlContext)\n",
    "# bq.setGcpJsonKeyFile(KEY_FILE)\n",
    "# bq.setBigQueryProjectId(BQ_PROJECT_ID)\n",
    "# bq.setGSProjectId(BQ_PROJECT_ID)\n",
    "# bq.setBigQueryGcsBucket(STAGING_BUCKET)\n",
    "# bq.setBigQueryDatasetLocation(DATASET_LOCATION)\n",
    "\n",
    "# # Extract and Transform a dataframe\n",
    "# # df = session.read.csv(...)\n",
    "\n",
    "# # Load into a table or table partition\n",
    "# bqDF = bigquery.BigQueryDataFrame(df_gr._jdf)\n",
    "# bqDF.saveAsBigQueryTable(\n",
    "#     \"{0}:{1}.{2}\".format(BQ_PROJECT_ID, DATASET_ID, TABLE_NAME),\n",
    "#     False, # Day paritioned when created\n",
    "#     0,     # Partition expired when created\n",
    "#     bigquery.__getattr__(\"package$WriteDisposition$\").__getattr__(\"MODULE$\").WRITE_EMPTY(),\n",
    "#     bigquery.__getattr__(\"package$CreateDisposition$\").__getattr__(\"MODULE$\").CREATE_IF_NEEDED(),\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d2cd75694f64a89ce38a33c09feb63608218b860153f71811653dc2ba7e169fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
